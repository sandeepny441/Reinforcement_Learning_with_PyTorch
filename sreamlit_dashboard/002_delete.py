# -*- coding: utf-8 -*-
"""Chapter 05 - Build_a_News_Articles_Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2005%20-%20Build_a_News_Articles_Summarizer.ipynb
"""

# Install necessary packages
!pip install -q langchain==0.0.208 openai==0.27.8 python-dotenv newspaper3k

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load environment variables
from dotenv import load_dotenv

# Add your OpenAI API key here
!echo "OPENAI_API_KEY='<OPENAI_API_KEY>'" > .env

load_dotenv()

# Import necessary libraries
import requests
from newspaper import Article

# Define headers for web scraping
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'
}

# URL of the article to summarize
article_urls = "https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/"

# Initialize a session
session = requests.Session()

try:
    # Fetch the article
    response = session.get(article_urls, headers=headers, timeout=10)

    if response.status_code == 200:
        # Use Newspaper3k to download and parse the article
        article = Article(article_urls)
        article.download()
        article.parse()

        # Print the article title and text
        print(f"Title: {article.title}")
        print(f"Text: {article.text}")

    else:
        print(f"Failed to fetch article at {article_urls}")
except Exception as e:
    print(f"Error occurred while fetching article at {article_urls}: {e}")

# Import LangChain schema components
from langchain.schema import (
    HumanMessage
)

# Extract article data
article_title = article.title
article_text = article.text

# Prepare template for summarization prompt
template = """You are a very good assistant that summarizes online articles.

Here's the article you want to summarize.

==================
Title: {article_title}

{article_text}
==================

Write a summary of the previous article.
"""

# Format the prompt with the article details
prompt = template.format(article_title=article.title, article_text=article.text)

# Create a HumanMessage for LangChain
messages = [HumanMessage(content=prompt)]

# Import and initialize the ChatOpenAI model
from langchain.chat_models import ChatOpenAI

# Load the model with specified parameters
chat = ChatOpenAI(model_name="gpt-4-turbo", temperature=0)

# Generate summary
summary = chat(messages)
print(summary.content)

# Prepare template for bulleted list summarization
template = """You are an advanced AI assistant that summarizes online articles into bulleted lists.

Here's the article you need to summarize.

==================
Title: {article_title}

{article_text}
==================

Now, provide a summarized version of the article in a bulleted list format.
"""

# Format the prompt with the article details
prompt = template.format(article_title=article.title, article_text=article.text)

# Generate bulleted summary
summary = chat([HumanMessage(content=prompt)])
print(summary.content)

# Prepare template for bulleted list summarization in French
template = """You are an advanced AI assistant that summarizes online articles into bulleted lists in French.

Here's the article you need to summarize.

==================
Title: {article_title}

{article_text}
==================

Now, provide a summarized version of the article in a bulleted list format, in French.
"""

# Format the prompt with the article details
prompt = template.format(article_title=article.title, article_text=article.text)

# Generate bulleted summary in French
summary = chat([HumanMessage(content=prompt)])
print(summary.content)


# Display results
print("Price-Based Features:")
print(price_features.head())

# src/my_project/config/tasks.yaml
research_task:
  description: >
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2024.
  expected_output: >
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: >
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: >
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without '```'
  agent: reporting_analyst
  output_file: report.md



  import pandas as pd
from textblob import TextBlob
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import re

# Sample tweets
tweets = [
    "I love the new features on the app! Great job by the team.",
    "This update is terrible, nothing works as expected.",
    "The customer service was so helpful and polite. Kudos!",
    "Why does the app crash every time I open it? Frustrating!",
    "The new UI design is sleek and user-friendly.",
    "Terrible experience with the latest version, please fix ASAP.",
    "Thank you for resolving my issue so quickly. Impressed!",
    "This app has gone from bad to worse. Really disappointed.",
    "Loving the dark mode feature, much needed!",
    "I can't believe how bad the performance has become. Fix it!"
]

# Create a DataFrame
df = pd.DataFrame(tweets, columns=["Tweet"])

# Text Cleaning
def clean_text(text):
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"@\w+", "", text)    # Remove mentions
    text = re.sub(r"#\w+", "", text)    # Remove hashtags
    text = re.sub(r"[^\w\s]", "", text) # Remove punctuation
    return text.lower()

df["Cleaned_Tweet"] = df["Tweet"].apply(clean_text)

# Sentiment Analysis
def get_sentiment(text):
    analysis = TextBlob(text)
    return "Positive" if analysis.sentiment.polarity > 0 else "Negative" if analysis.sentiment.polarity < 0 else "Neutral"

df["Sentiment"] = df["Cleaned_Tweet"].apply(get_sentiment)

# Word Frequency
all_words = " ".join(df["Cleaned_Tweet"])
word_counts = Counter(all_words.split())
common_words = word_counts.most_common(10)

# WordCloud
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_words)

# Plotting
plt.figure(figsize=(14, 6))

# WordCloud
plt.subplot(1, 2, 1)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("WordCloud of Tweets")

# Sentiment Distribution
plt.subplot(1, 2, 2)
df["Sentiment"].value_counts().plot(kind="bar", color=["green", "red", "blue"])
plt.title("Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")

plt.tight_layout()
plt.show()

# Print insights
print("Top 10 Common Words:", common_words)
print("\nTweet Sentiments:")
print(df[["Tweet", "Sentiment"]])
